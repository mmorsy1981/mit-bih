{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97af3376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-19T10:46:44.374152Z",
     "iopub.status.busy": "2021-10-19T10:46:44.374152Z",
     "iopub.status.idle": "2021-10-19T10:46:48.231793Z",
     "shell.execute_reply": "2021-10-19T10:46:48.230851Z",
     "shell.execute_reply.started": "2021-10-19T10:46:44.374152Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from keras.layers.experimental import preprocessing\n",
    "from keras.layers import Conv1D, MaxPooling1D, BatchNormalization, Dropout, Input, Dense, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "#%pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1498752b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-19T10:46:48.232794Z",
     "iopub.status.busy": "2021-10-19T10:46:48.232794Z",
     "iopub.status.idle": "2021-10-19T10:46:49.140385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.138384Z",
     "shell.execute_reply.started": "2021-10-19T10:46:48.232794Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-27505717d707>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mphysical_devices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphysical_devices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Num GPUs Available: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112a0fa",
   "metadata": {},
   "source": [
    "Load Dataset\n",
    "\n",
    "https://physionet.org/content/mitdb/1.0.0/\n",
    "\n",
    "Background\n",
    "\n",
    "Since 1975, our laboratories at Boston's Beth Israel Hospital (now the Beth Israel Deaconess Medical Center) and at MIT have supported our own research into arrhythmia analysis and related subjects. One of the first major products of that effort was the MIT-BIH Arrhythmia Database, which we completed and began distributing in 1980. The database was the first generally available set of standard test material for evaluation of arrhythmia detectors, and has been used for that purpose as well as for basic research into cardiac dynamics at more than 500 sites worldwide. Originally, we distributed the database on 9-track half-inch digital tape at 800 and 1600 bpi, and on quarter-inch IRIG-format FM analog tape. In August, 1989, we produced a CD-ROM version of the database.\n",
    "\n",
    "Data Description\n",
    "\n",
    "The MIT-BIH Arrhythmia Database contains 48 half-hour excerpts of two-channel ambulatory ECG recordings, obtained from 47 subjects studied by the BIH Arrhythmia Laboratory between 1975 and 1979. Twenty-three recordings were chosen at random from a set of 4000 24-hour ambulatory ECG recordings collected from a mixed population of inpatients (about 60%) and outpatients (about 40%) at Boston's Beth Israel Hospital; the remaining 25 recordings were selected from the same set to include less common but clinically significant arrhythmias that would not be well-represented in a small random sample.\n",
    "\n",
    "The recordings were digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range. Two or more cardiologists independently annotated each record; disagreements were resolved to obtain the computer-readable reference annotations for each beat (approximately 110,000 annotations in all) included with the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baa1879",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.141383Z",
     "iopub.status.idle": "2021-10-19T10:46:49.141383Z",
     "shell.execute_reply": "2021-10-19T10:46:49.141383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Triaining and Testing Datasets\n",
    "# Local\n",
    "file_path = '../mitbihcsv'\n",
    "#Paperspace\n",
    "#file_path = '/storage/data/mit-bih'\n",
    "\n",
    "file_name = 'mit_bih_data_set.csv'\n",
    "dataset_path = os.path.join(file_path,file_name)\n",
    "data_set = pd.read_csv(dataset_path,dtype=float)\n",
    "num_classes=6\n",
    "data_set_np = data_set.to_numpy()\n",
    "X = data_set_np[:,:-1].astype('float32')\n",
    "y = data_set_np[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f773a5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.142392Z",
     "iopub.status.idle": "2021-10-19T10:46:49.142392Z",
     "shell.execute_reply": "2021-10-19T10:46:49.142392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size=0.2,stratify=y, random_state=42)\n",
    "\n",
    "y_trn_oh = to_categorical(y_trn)\n",
    "y_tst_oh = to_categorical(y_tst)\n",
    "\n",
    "X_trn_tf = np.expand_dims(X_trn, axis=2)\n",
    "X_tst_tf = np.expand_dims(X_tst, axis=2)\n",
    "\n",
    "\n",
    "#X_tst = X_tst.astype('float32')\n",
    "#X_tst_tf = X_tst_tf.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db91b71",
   "metadata": {},
   "source": [
    "## 3. Training a 1D Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f44de30",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.143387Z",
     "iopub.status.idle": "2021-10-19T10:46:49.143387Z",
     "shell.execute_reply": "2021-10-19T10:46:49.143387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 500\n",
    "validation_split = 0.2 # 20% of training set will be used for validation set. \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(16, 8, activation='relu', padding=\"same\",\n",
    "                 input_shape=X_trn_tf[1,:].shape,\n",
    "                 name='Conv1DLayer1'))\n",
    "model.add(MaxPooling1D(4, padding='same'))\n",
    "    \n",
    "model.add(Conv1D(16, 8, activation='relu', padding=\"same\", name='Conv1DLayer2'))\n",
    "model.add(MaxPooling1D(4, padding='same'))\n",
    "    \n",
    "model.add(Conv1D(16, 16, activation='relu', padding=\"same\", name='Conv1DLayer3'))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(num_classes, activation='softmax', name = 'OuputLayer'))\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,\n",
    "                                                             decay_steps=1000,decay_rate=0.9)\n",
    "\n",
    "opt = RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "#tboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logs,\n",
    "#                                                 histogram_freq = 1,\n",
    "#                                                 profile_batch = '30,80')\n",
    "\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=50),\n",
    "             ModelCheckpoint(filepath='./base_model.h5', \n",
    "                             monitor='val_accuracy', save_best_only=True)]\n",
    "             #, tboard_callback]\n",
    "\n",
    "# fit network\n",
    "history=model.fit(X_trn_tf, y_trn_oh, epochs=epochs,callbacks=callbacks, \n",
    "                  batch_size=batch_size,validation_split=validation_split)\n",
    "\n",
    "model.load_weights('base_model.h5')\n",
    "\n",
    "train_score = model.evaluate(X_trn_tf, y_trn_oh)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = model.evaluate(X_tst_tf, y_tst_oh)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05e3e0",
   "metadata": {},
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036053cb",
   "metadata": {},
   "source": [
    "# Launch TensorBoard and navigate to the Profile tab to view performance profile\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de63fdc4",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.145385Z",
     "iopub.status.idle": "2021-10-19T10:46:49.145385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.145385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_model(history,X_test,y_test,model):\n",
    "    scores = model.evaluate((X_test),y_test, verbose=0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    \n",
    "    print(history)\n",
    "    fig1, ax_acc = plt.subplots()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model - Accuracy')\n",
    "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    fig2, ax_loss = plt.subplots()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Model- Loss')\n",
    "    plt.legend(['Training', 'Validation'], loc='upper right')\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.show()\n",
    "    target_names=['0','1','2','3','4','5']\n",
    "    \n",
    "    y_true=[]\n",
    "    for element in y_test:\n",
    "        y_true.append(np.argmax(element))\n",
    "    prediction_proba=model.predict(X_test)\n",
    "    prediction=np.argmax(prediction_proba,axis=1)\n",
    "    cnf_matrix = confusion_matrix(y_true, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c833048a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.146392Z",
     "iopub.status.idle": "2021-10-19T10:46:49.146392Z",
     "shell.execute_reply": "2021-10-19T10:46:49.146392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.config.experimental.get_memory_usage(\"GPU:0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dd5b73",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.147391Z",
     "iopub.status.idle": "2021-10-19T10:46:49.147391Z",
     "shell.execute_reply": "2021-10-19T10:46:49.147391Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "evaluate_model(history,X_tst_tf,y_tst_oh,model)\n",
    "y_pred=model.predict(X_tst_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e09cb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.148385Z",
     "iopub.status.idle": "2021-10-19T10:46:49.148385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.148385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    start = time.time()\n",
    "    model.predict(X_tst_tf)\n",
    "    end = time.time()\n",
    "\n",
    "inference_time =  (end-start)/X_tst_tf.shape[0]\n",
    "throughput = X_tst_tf.shape[0]/(end-start)\n",
    "\n",
    "print(\"Inference on CPU is :\", inference_time)\n",
    "print(\"Throughput on CPU is :\", throughput, \"sequence per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99ba87d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.149383Z",
     "iopub.status.idle": "2021-10-19T10:46:49.149383Z",
     "shell.execute_reply": "2021-10-19T10:46:49.149383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    start = time.time()\n",
    "    model.predict(X_tst_tf)\n",
    "    end = time.time()\n",
    "\n",
    "inference_time =  (end-start)/X_tst_tf.shape[0]\n",
    "throughput = X_tst_tf.shape[0]/(end-start)\n",
    "\n",
    "print(\"Inference on GPU is :\", inference_time)\n",
    "print(\"Throughput on GPU is :\", throughput, \"sequence per second\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664eaeab",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.149383Z",
     "iopub.status.idle": "2021-10-19T10:46:49.149383Z",
     "shell.execute_reply": "2021-10-19T10:46:49.149383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_tst_oh.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(10, 10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['N', 'L', 'R', 'A', 'V', '/'],normalize=True,\n",
    "                      title='Confusion matrix, with normalization')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b817a0b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.150383Z",
     "iopub.status.idle": "2021-10-19T10:46:49.151384Z",
     "shell.execute_reply": "2021-10-19T10:46:49.150383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import plotting\n",
    "\n",
    "classes = ['N', 'L', 'R', 'A', 'V', '/']\n",
    "\n",
    "y_keras = model.predict(X_tst_tf)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_tst_oh, axis=1), np.argmax(y_keras, axis=1))))\n",
    "plt.figure(figsize=(9,9))\n",
    "_ = plotting.makeRoc(y_tst_oh, y_keras, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f3ff2",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.151384Z",
     "iopub.status.idle": "2021-10-19T10:46:49.151384Z",
     "shell.execute_reply": "2021-10-19T10:46:49.151384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model,show_shapes=True, show_dtype=True)\n",
    "#tf.keras.utils.plot_model(model, to_file='model.png', show_shapes=False, show_dtype=False, show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64969e63",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.153384Z",
     "iopub.status.idle": "2021-10-19T10:46:49.153384Z",
     "shell.execute_reply": "2021-10-19T10:46:49.153384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install tf_agents==0.7.1\n",
    "#!pip install tensorflow_probability==0.12.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf893dd",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.154386Z",
     "iopub.status.idle": "2021-10-19T10:46:49.154386Z",
     "shell.execute_reply": "2021-10-19T10:46:49.154386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from plot_model import plot_model\n",
    "plot_model(model,to_file='base_model.png', show_shapes=True, show_layer_names=False, rankdir='TB', expand_nested=False, style=0, color=True, dpi=96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e49ec35",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.155386Z",
     "iopub.status.idle": "2021-10-19T10:46:49.156385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.156385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea95a6d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T18:40:14.417050Z",
     "iopub.status.busy": "2021-10-10T18:40:14.416054Z",
     "iopub.status.idle": "2021-10-10T18:40:14.430058Z",
     "shell.execute_reply": "2021-10-10T18:40:14.429050Z",
     "shell.execute_reply.started": "2021-10-10T18:40:14.417050Z"
    }
   },
   "source": [
    "# Tensorflow Optimization Toolkit does not support Conv1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b98115",
   "metadata": {},
   "source": [
    "## Convert best model to tflite with floating points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fb1d42",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.156385Z",
     "iopub.status.idle": "2021-10-19T10:46:49.156385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.156385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "base_model_size = os.stat('base_model.h5').st_size / 1024\n",
    "print('Base model size = %dKBs.' % base_model_size)\n",
    "\n",
    "tflite_model_size = len(tflite_model) / 1024\n",
    "print('tflite model size = %dKBs.' % tflite_model_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc83fe26",
   "metadata": {},
   "source": [
    "## Run tflite interpreter to compute inference result of specific input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3277888",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.158386Z",
     "iopub.status.idle": "2021-10-19T10:46:49.158386Z",
     "shell.execute_reply": "2021-10-19T10:46:49.158386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "#get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "#Preprocess the image to required size and cast\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.expand_dims(X_tst_tf[0], 0)\n",
    "\n",
    "#set the tensor to point to the input data to be inferred\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "#Run the inference\n",
    "interpreter.invoke()\n",
    "output_details = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d7efce",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.159391Z",
     "iopub.status.idle": "2021-10-19T10:46:49.159391Z",
     "shell.execute_reply": "2021-10-19T10:46:49.159391Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A helper function to evaluate the TF Lite model using \"test\" dataset.\n",
    "def evaluate_tflite_model(tflite_model):\n",
    "  # Initialize TFLite interpreter using the model.\n",
    "  interpreter = tf.lite.Interpreter(model_content=float_tflite_model)\n",
    "  interpreter.allocate_tensors()\n",
    "  input_tensor_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output = interpreter.tensor(interpreter.get_output_details()[0][\"index\"])\n",
    "\n",
    "  # Run predictions on every image in the \"test\" dataset.\n",
    "  prediction_outputs = []\n",
    "  for data in X_tst_tf:\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_data = np.expand_dims(data, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_tensor_index, test_data)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    prediction = np.argmax(output()[0])\n",
    "    prediction_outputs.append(prediction)\n",
    "\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  accurate_count = 0\n",
    "  for index in range(len(prediction_outputs)):\n",
    "    if prediction_outputs[index] == y_tst[index]:\n",
    "      accurate_count += 1\n",
    "  accuracy = accurate_count * 1.0 / len(prediction_outputs)\n",
    "\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09722ff8",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.161388Z",
     "iopub.status.idle": "2021-10-19T10:46:49.161388Z",
     "shell.execute_reply": "2021-10-19T10:46:49.161388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the TF Lite float model. You'll find that its accurary is identical\n",
    "# to the original TF (Keras) model because they are essentially the same model\n",
    "# stored in different format.\n",
    "tflite_accuracy = evaluate_tflite_model(tflite_model)\n",
    "print('tflite model accuracy = %.4f' % tflite_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa049cf1",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.162388Z",
     "iopub.status.idle": "2021-10-19T10:46:49.162388Z",
     "shell.execute_reply": "2021-10-19T10:46:49.162388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the tflite model to file to the Downloads directory\n",
    "f = open('float32.tflite', \"wb\")\n",
    "f.write(tflite_model)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f05f24d",
   "metadata": {},
   "source": [
    "## Dynamic range quantization of tflite from best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a7e5d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.163386Z",
     "iopub.status.idle": "2021-10-19T10:46:49.164386Z",
     "shell.execute_reply": "2021-10-19T10:46:49.164386Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Re-convert the model to TF Lite using quantization.\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Set float16 is the supported type on the target platform\n",
    "converter.target_spec.supported_types = [tf.float16]\n",
    "\n",
    "tflite_qmodel = converter.convert()\n",
    "\n",
    "# Show model size in KBs.\n",
    "quantized_model_size = len(tflite_qmodel) / 1024\n",
    "print('Quantized model size = %dKBs,' % quantized_model_size)\n",
    "print('which is about %d%% of the float model size.'\\\n",
    "      % (quantized_model_size * 100 / tflite_model_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0f4505",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.165385Z",
     "iopub.status.idle": "2021-10-19T10:46:49.165385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.165385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_qmodel)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "#get input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "#Preprocess the image to required size and cast\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.expand_dims(X_tst_tf[0], 0)\n",
    "\n",
    "#set the tensor to point to the input data to be inferred\n",
    "input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "\n",
    "#Run the inference\n",
    "interpreter.invoke()\n",
    "output_details = interpreter.get_tensor(output_details[0]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba9e399",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.166384Z",
     "iopub.status.idle": "2021-10-19T10:46:49.166384Z",
     "shell.execute_reply": "2021-10-19T10:46:49.166384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the quantized model to file to the Downloads directory\n",
    "f = open('quant_float16.tflite', \"wb\")\n",
    "f.write(tflite_qmodel)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2312cf",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.167385Z",
     "iopub.status.idle": "2021-10-19T10:46:49.167385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.167385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evalualte the TF Lite quantized model.\n",
    "# Don't be surprised if you see quantized model accuracy is higher than\n",
    "# the original float model. It happens sometimes :)\n",
    "\n",
    "quantized_tflite_accuracy = evaluate_tflite_model(tflite_qmodel)\n",
    "print('Quantized float 16 model accuracy = %.4f' % quantized_tflite_accuracy)\n",
    "\n",
    "print('Accuracy drop = %.4f' % (tflite_accuracy - quantized_tflite_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3951f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T18:29:51.954141Z",
     "iopub.status.busy": "2021-10-10T18:29:51.954141Z",
     "iopub.status.idle": "2021-10-10T18:29:51.965056Z",
     "shell.execute_reply": "2021-10-10T18:29:51.963970Z",
     "shell.execute_reply.started": "2021-10-10T18:29:51.954141Z"
    }
   },
   "source": [
    "## Int8 Qkeras Quantization of tflite model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fa378c",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.168384Z",
     "iopub.status.idle": "2021-10-19T10:46:49.169386Z",
     "shell.execute_reply": "2021-10-19T10:46:49.168384Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "from qkeras import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d916458",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.170385Z",
     "iopub.status.idle": "2021-10-19T10:46:49.170385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.170385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CreateQModel(X_train,y_train,X_test,y_test):\n",
    "    X = x_in = Input(shape = X_train[1,:].shape)\n",
    "    \n",
    "    #quantized_bits(bits, integer, symmetric, keep_negative, alpha, use_stochastic_rouding)\n",
    "\n",
    "    X = QConv1D(16, 8,padding=\"same\",\n",
    "                      kernel_quantizer=\"quantized_bits(8, 0, 1)\", \n",
    "                      bias_quantizer=\"quantized_bits(8, 0, 1)\",\n",
    "                      name=\"Qconv1d_1\")(X)\n",
    "    X = QActivation(\"quantized_relu(8,0)\")(X)\n",
    "    X = MaxPooling1D(4, padding='same')(X)\n",
    "\n",
    "    X = QConv1D(16, 8,padding=\"same\",\n",
    "                      kernel_quantizer=\"quantized_bits(8, 0, 1)\", \n",
    "                      bias_quantizer=\"quantized_bits(8, 0, 1)\",\n",
    "                      name=\"Qconv1d_2\")(X)\n",
    "    X = QActivation(\"quantized_relu(8,0)\")(X)\n",
    "    X = MaxPooling1D(4, padding='same')(X)\n",
    "    \n",
    "    X = QConv1D(16, 8,padding=\"same\",\n",
    "                      kernel_quantizer=\"quantized_bits(8, 0, 1)\", \n",
    "                      bias_quantizer=\"quantized_bits(8, 0, 1)\",\n",
    "                      name=\"Qconv1d_3\")(X)\n",
    "    X = QActivation(\"quantized_relu(8,0)\")(X)\n",
    "\n",
    "    X = Flatten()(X)\n",
    "\n",
    "    X = QDense(num_classes, \n",
    "               kernel_quantizer=\"quantized_bits(8, 0, 1)\", \n",
    "               bias_quantizer=\"quantized_bits(8, 0, 1)\",\n",
    "               name = 'OuputLayer')(X)\n",
    "    X = Activation(\"softmax\")(X)\n",
    "    \n",
    "    qmodel = Model(inputs=x_in, outputs=X)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,\n",
    "                                                             decay_steps=1000,decay_rate=0.9)\n",
    "    opt = RMSprop(learning_rate=lr_schedule)\n",
    "    \n",
    "    qmodel.compile(loss='categorical_crossentropy', optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=50),\n",
    "                 ModelCheckpoint(filepath='./qkeras_int16_model.h5', \n",
    "                                 monitor='val_accuracy', save_best_only=True)]\n",
    "\n",
    "\n",
    "    # fit network\n",
    "    history=qmodel.fit(X_train, y_train, epochs=epochs,callbacks=callbacks, \n",
    "                      batch_size=batch_size,validation_split=validation_split,shuffle=True)\n",
    "\n",
    "    qmodel.load_weights('qkeras_int16_model.h5')\n",
    "\n",
    "    train_score = qmodel.evaluate(X_trn_tf, y_trn_oh)\n",
    "    print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "    test_score = qmodel.evaluate(X_tst_tf, y_tst_oh)\n",
    "    print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))\n",
    "\n",
    "    return(qmodel,history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a3772e",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.171385Z",
     "iopub.status.idle": "2021-10-19T10:46:49.171385Z",
     "shell.execute_reply": "2021-10-19T10:46:49.171385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qmodel,history = CreateQModel(X_trn_tf,y_trn_oh,X_tst_tf,y_tst_oh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0a7cdb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.172392Z",
     "iopub.status.idle": "2021-10-19T10:46:49.172392Z",
     "shell.execute_reply": "2021-10-19T10:46:49.172392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "qmodel.summary()\n",
    "\n",
    "print_qstats(qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f8ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-19T09:38:02.520977Z",
     "iopub.status.busy": "2021-10-19T09:38:02.519978Z",
     "iopub.status.idle": "2021-10-19T09:38:02.820494Z",
     "shell.execute_reply": "2021-10-19T09:38:02.819493Z",
     "shell.execute_reply.started": "2021-10-19T09:38:02.520977Z"
    },
    "tags": []
   },
   "source": [
    "# Commented Code\n",
    "from qkeras.utils import *\n",
    "model_save_quantized_weights(qmodel, 'qmodel.h5')\n",
    "#load_qmodel('qmodel.h5', custom_objects=None, compile=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdc583d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.174388Z",
     "iopub.status.idle": "2021-10-19T10:46:49.174388Z",
     "shell.execute_reply": "2021-10-19T10:46:49.174388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qkeras.utils import *\n",
    "\n",
    "print_model_sparsity(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20241813",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.176390Z",
     "iopub.status.idle": "2021-10-19T10:46:49.177410Z",
     "shell.execute_reply": "2021-10-19T10:46:49.177410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "quantized_model_debug(qmodel, X_tst_tf, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc59802f",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.180387Z",
     "iopub.status.idle": "2021-10-19T10:46:49.180387Z",
     "shell.execute_reply": "2021-10-19T10:46:49.180387Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "extract_model_operations(qmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f087ef5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-19T09:39:06.104406Z",
     "iopub.status.busy": "2021-10-19T09:39:06.103402Z",
     "iopub.status.idle": "2021-10-19T09:39:06.345409Z",
     "shell.execute_reply": "2021-10-19T09:39:06.344438Z",
     "shell.execute_reply.started": "2021-10-19T09:39:06.104406Z"
    },
    "tags": []
   },
   "source": [
    "# Commented Code\n",
    "_, keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(qmodel, keras_file, include_optimizer=False)\n",
    "print('Saved baseline model to:', keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eba3fae",
   "metadata": {},
   "source": [
    "## Prune QKeras Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964804ff",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.181390Z",
     "iopub.status.idle": "2021-10-19T10:46:49.182392Z",
     "shell.execute_reply": "2021-10-19T10:46:49.182392Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_callbacks\n",
    "from tensorflow_model_optimization.python.core.sparsity.keras import pruning_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f6a23d",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.183389Z",
     "iopub.status.idle": "2021-10-19T10:46:49.184388Z",
     "shell.execute_reply": "2021-10-19T10:46:49.184388Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
    "\n",
    "num_examples = X_trn_tf.shape[0] * (1 - validation_split)\n",
    "end_step = np.ceil(num_examples / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "# Define model for pruning.\n",
    "pruning_params = {\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                               final_sparsity=0.90,\n",
    "                                                               begin_step=0,\n",
    "                                                               end_step=end_step)\n",
    "}\n",
    "\n",
    "pqmodel = prune_low_magnitude(qmodel, **pruning_params)\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=1e-2,\n",
    "                                                             decay_steps=1000,decay_rate=0.9)\n",
    "opt = RMSprop(learning_rate=lr_schedule)\n",
    "    \n",
    "pqmodel.compile(loss='categorical_crossentropy', optimizer=opt, \n",
    "                  metrics=['accuracy'])\n",
    "pqmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25e08a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-10-19T10:46:49.185389Z",
     "iopub.status.idle": "2021-10-19T10:46:49.186389Z",
     "shell.execute_reply": "2021-10-19T10:46:49.186389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "with TemporaryDirectory() as temp_dir:\n",
    "\n",
    "    callbacks = [\n",
    "      tfmot.sparsity.keras.UpdatePruningStep(),\n",
    "      tfmot.sparsity.keras.PruningSummaries(log_dir=temp_dir),\n",
    "      EarlyStopping(monitor='val_loss', patience=50),\n",
    "      ModelCheckpoint(filepath='./int8_pqmodel.h5', \n",
    "      monitor='val_accuracy', save_best_only=True)\n",
    "    ]\n",
    "\n",
    "    # fit network\n",
    "    history=pqmodel.fit(X_trn_tf, y_trn_oh, epochs=epochs,callbacks=callbacks, \n",
    "                          batch_size=batch_size, validation_split=validation_split,shuffle=True)\n",
    "\n",
    "pqmodel.load_weights('int8_pqmodel.h5')\n",
    "\n",
    "train_score = pqmodel.evaluate(X_trn_tf, y_trn_oh)\n",
    "print('Train loss: {}, Train accuracy: {}'.format(train_score[0], train_score[1]))\n",
    "test_score = pqmodel.evaluate(X_tst_tf, y_tst_oh)\n",
    "print('Test loss: {}, Test accuracy: {}'.format(test_score[0], test_score[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae14928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-19T10:45:57.138068Z",
     "iopub.status.busy": "2021-10-19T10:45:57.138068Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "_, pq_model_accuracy = pqmodel.evaluate(X_tst_tf, y_tst_oh, verbose=0)\n",
    "print('Pruned test accuracy:', pq_model_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2288d621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T20:19:31.691604Z",
     "iopub.status.busy": "2021-10-10T20:19:31.690580Z",
     "iopub.status.idle": "2021-10-10T20:19:32.417360Z",
     "shell.execute_reply": "2021-10-10T20:19:32.416337Z",
     "shell.execute_reply.started": "2021-10-10T20:19:31.691604Z"
    },
    "tags": []
   },
   "source": [
    "# Commented Code\n",
    "model_for_export = tfmot.sparsity.keras.strip_pruning(p_q_model)\n",
    "\n",
    "_, p_q_keras_file = tempfile.mkstemp('.h5')\n",
    "tf.keras.models.save_model(model_for_export, p_q_keras_file, include_optimizer=False)\n",
    "print('Saved pruned Keras model to:', p_q_keras_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41ac3f",
   "metadata": {},
   "source": [
    "## Convert Pruned Quantized Model to tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23ba792",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "p_q_tflite_model = converter.convert()\n",
    "\n",
    "_, p_q_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(p_q_tflite_file, 'wb') as f:\n",
    "  f.write(p_q_tflite_model)\n",
    "\n",
    "print('Saved pruned TFLite model to:', p_q_tflite_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d6fff48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T20:17:32.424640Z",
     "iopub.status.busy": "2021-10-10T20:17:32.424640Z",
     "iopub.status.idle": "2021-10-10T20:17:32.433639Z",
     "shell.execute_reply": "2021-10-10T20:17:32.432636Z",
     "shell.execute_reply.started": "2021-10-10T20:17:32.424640Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_gzipped_model_size(file):\n",
    "  # Returns size of gzipped model, in bytes.\n",
    "  import os\n",
    "  import zipfile\n",
    "\n",
    "  _, zipped_file = tempfile.mkstemp('.zip')\n",
    "  with zipfile.ZipFile(zipped_file, 'w', compression=zipfile.ZIP_DEFLATED) as f:\n",
    "    f.write(file)\n",
    "\n",
    "  return os.path.getsize(zipped_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4491fb6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T20:20:05.853011Z",
     "iopub.status.busy": "2021-10-10T20:20:05.852012Z",
     "iopub.status.idle": "2021-10-10T20:20:05.899009Z",
     "shell.execute_reply": "2021-10-10T20:20:05.898010Z",
     "shell.execute_reply.started": "2021-10-10T20:20:05.853011Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of gzipped baseline Keras model: 12923.00 bytes\n",
      "Size of gzipped pruned Keras model: 9864.00 bytes\n",
      "Size of gzipped pruned TFlite model: 7312.00 bytes\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned Keras model: %.2f bytes\" % (get_gzipped_model_size(p_q_keras_file)))\n",
    "print(\"Size of gzipped pruned TFlite model: %.2f bytes\" % (get_gzipped_model_size(p_q_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "851e5891",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T20:22:00.576311Z",
     "iopub.status.busy": "2021-10-10T20:22:00.576311Z",
     "iopub.status.idle": "2021-10-10T20:22:12.216995Z",
     "shell.execute_reply": "2021-10-10T20:22:12.215994Z",
     "shell.execute_reply.started": "2021-10-10T20:22:00.576311Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\mmorsy\\AppData\\Local\\Temp\\tmphjmqch2q\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\mmorsy\\AppData\\Local\\Temp\\tmphjmqch2q\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "to_proto not supported in EAGER mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "to_proto not supported in EAGER mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "to_proto not supported in EAGER mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "to_proto not supported in EAGER mode.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved quantized and pruned TFLite model to: C:\\Users\\mmorsy\\AppData\\Local\\Temp\\tmpssapwpku.tflite\n",
      "Size of gzipped baseline Keras model: 12923.00 bytes\n",
      "Size of gzipped pruned and quantized TFlite model: 6573.00 bytes\n"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "quantized_and_pruned_tflite_model = converter.convert()\n",
    "\n",
    "_, quantized_and_pruned_tflite_file = tempfile.mkstemp('.tflite')\n",
    "\n",
    "with open(quantized_and_pruned_tflite_file, 'wb') as f:\n",
    "  f.write(quantized_and_pruned_tflite_model)\n",
    "\n",
    "print('Saved quantized and pruned TFLite model to:', quantized_and_pruned_tflite_file)\n",
    "\n",
    "print(\"Size of gzipped baseline Keras model: %.2f bytes\" % (get_gzipped_model_size(keras_file)))\n",
    "print(\"Size of gzipped pruned and quantized TFlite model: %.2f bytes\" % (get_gzipped_model_size(quantized_and_pruned_tflite_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "08b566ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T20:22:32.109739Z",
     "iopub.status.busy": "2021-10-10T20:22:32.109739Z",
     "iopub.status.idle": "2021-10-10T20:22:32.119734Z",
     "shell.execute_reply": "2021-10-10T20:22:32.118768Z",
     "shell.execute_reply.started": "2021-10-10T20:22:32.109739Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_model(interpreter):\n",
    "  input_index = interpreter.get_input_details()[0][\"index\"]\n",
    "  output_index = interpreter.get_output_details()[0][\"index\"]\n",
    "\n",
    "  # Run predictions on ever y image in the \"test\" dataset.\n",
    "  prediction_results = []\n",
    "  for i, test_example in enumerate(X_tst_tf):\n",
    "    if i % 1000 == 0:\n",
    "      print('Evaluated on {n} results so far.'.format(n=i))\n",
    "    # Pre-processing: add batch dimension and convert to float32 to match with\n",
    "    # the model's input data format.\n",
    "    test_example = np.expand_dims(test_example, axis=0).astype(np.float32)\n",
    "    interpreter.set_tensor(input_index, test_example)\n",
    "\n",
    "    # Run inference.\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Post-processing: remove batch dimension and find the digit with highest\n",
    "    # probability.\n",
    "    output = interpreter.tensor(output_index)\n",
    "    digit = np.argmax(output()[0])\n",
    "    prediction_results.append(digit)\n",
    "\n",
    "  print('\\n')\n",
    "  # Compare prediction results with ground truth labels to calculate accuracy.\n",
    "  prediction_digits = np.array(prediction_results)\n",
    "  accuracy = (prediction_results == y_tst).mean()\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "491cb2e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T20:22:36.855941Z",
     "iopub.status.busy": "2021-10-10T20:22:36.855941Z",
     "iopub.status.idle": "2021-10-10T20:22:48.337735Z",
     "shell.execute_reply": "2021-10-10T20:22:48.337735Z",
     "shell.execute_reply.started": "2021-10-10T20:22:36.855941Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated on 0 results so far.\n",
      "Evaluated on 1000 results so far.\n",
      "Evaluated on 2000 results so far.\n",
      "Evaluated on 3000 results so far.\n",
      "Evaluated on 4000 results so far.\n",
      "Evaluated on 5000 results so far.\n",
      "Evaluated on 6000 results so far.\n",
      "Evaluated on 7000 results so far.\n",
      "Evaluated on 8000 results so far.\n",
      "\n",
      "\n",
      "Pruned and quantized TFLite test_accuracy: 0.23786292241789625\n",
      "Pruned TF test accuracy: 0.9654926061630249\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_content=quantized_and_pruned_tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "test_accuracy = evaluate_model(interpreter)\n",
    "\n",
    "print('Pruned and quantized TFLite test_accuracy:', test_accuracy)\n",
    "print('Pruned TF test accuracy:', model_for_pruning_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5bee70ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-10T18:08:38.255180Z",
     "iopub.status.busy": "2021-10-10T18:08:38.255180Z",
     "iopub.status.idle": "2021-10-10T18:08:38.949238Z",
     "shell.execute_reply": "2021-10-10T18:08:38.949238Z",
     "shell.execute_reply.started": "2021-10-10T18:08:38.255180Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mmorsy\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\profiler\\internal\\flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mmorsy\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\profiler\\internal\\flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16392"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_flops(model):\n",
    "    session = tf.compat.v1.Session()\n",
    "    graph = tf.compat.v1.get_default_graph()   \n",
    "\n",
    "    with graph.as_default():\n",
    "        with session.as_default():\n",
    "            model = tf.keras.models.load_model(model)\n",
    "\n",
    "            run_meta = tf.compat.v1.RunMetadata()\n",
    "            opts = tf.compat.v1.profiler.ProfileOptionBuilder.float_operation()\n",
    "        \n",
    "            # We use the Keras session graph in the call to the profiler.\n",
    "            flops = tf.compat.v1.profiler.profile(graph=graph,\n",
    "                                                  run_meta=run_meta, cmd='op', options=opts)\n",
    "        \n",
    "            return flops.total_float_ops\n",
    "\n",
    "get_flops('best_model_Conv1D.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
